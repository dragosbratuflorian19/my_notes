{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch packages:\n",
    "- torch: the top level packages\n",
    "- torch.nn: contains layers, weights...\n",
    "- torch.autograd: handles the derivative calculations\n",
    "- torch.nn.functional: loss functions/activation functions/convolution operations\n",
    "- torch.optim: optimization algorithms: SGD/Adam\n",
    "- torch.utils: datasets/dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA:\n",
    "- GPU are better than CPU if the computation can be done in paralel (CPUs can have 4/8/16 cores, comparing to GPUs which can have thousands of cores(higher GPUs have â‰ˆ3000 cores))\n",
    "- NN are embarassingly parallel (could be easly broken into smaller computations: e.g.: Convolution operation)\n",
    "- CUDA is a SW platform that pairs with the GPU platform\n",
    "- CUDA is the SW layer that provides an API to the developers\n",
    "- Don't use CUDA for simple tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "# tensor([[1, 2, 3],[4, 5, 6]]) is on CPU, by default\n",
    "t = t.cuda()\n",
    "# tensor([[1, 2, 3],[4, 5, 6]], device='cuda:0') is on GPU (the first GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors:\n",
    "\n",
    "- number = scalar\n",
    "- array = vector\n",
    "- 2d-array = matrix\n",
    "- nd-tensor = nd-array\n",
    "\n",
    "- Rank of a tensor: the number of dimensions (matrix has rank 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "\n",
    "t.shape \n",
    "# torch.Size([2, 3])\n",
    "\n",
    "t.reshape(1, 6).shape\n",
    "# torch.Size([1, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "t = torch.Tensor()\n",
    "print(t.dtype)\n",
    "# torch.float\n",
    "print(t.device)\n",
    "# cpu\n",
    "print(t.layout)\n",
    "# torch.strided : how our tensors data is laid out in memory\n",
    "device = torch.device('cuda:0')\n",
    "# device(type='cuda', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([1., 2., 3.])\n",
    "t1 + t2 # ERROR\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2= t1.cuda()\n",
    "t1 + t2 # ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, 3])\n",
    "\n",
    "torch.Tensor(data) \n",
    "# tensor([1., 2., 3.])\n",
    "\n",
    "torch.tensor(data)\n",
    "# tensor([1, 2, 3], dtype=torch,int32)\n",
    "\n",
    "torch.as_tensor(data)\n",
    "# tensor([1, 2, 3], dtype=torch,int32)\n",
    "\n",
    "torch.from_numpy(data)\n",
    "# tensor([1, 2, 3], dtype=torch,int32)\n",
    "\n",
    "torch.eye() # identity tensor\n",
    "# 1 0\n",
    "# 0 1\n",
    "torch.zeros(2, 2)\n",
    "# 0 0\n",
    "# 0 0\n",
    "torch.ones(2, 2)\n",
    "# 1 1\n",
    "# 1 1\n",
    "torch.rand(2, 2)\n",
    "# 0.312 0.652\n",
    "# 0.452 0.912"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, 3])\n",
    "\n",
    "torch.Tensor(data)\n",
    "# tensor([1., 2., 3.]) # Class constructor\n",
    "\n",
    "torch.tensor(data)\n",
    "# tensor([1, 2, 3], dtype=torch,int32) # Factory function ( also as_tensor, from_numpy) -> Prefered \n",
    "\n",
    "torch.as_tensor(data)\n",
    "# tensor([1, 2, 3], dtype=torch,int32)\n",
    "\n",
    "torch.from_numpy(data)\n",
    "# tensor([1, 2, 3], dtype=torch,int32)\n",
    "\n",
    "# Set the data type\n",
    "torch.tensor(np.array([1, 2, 3]), dtype=torch.float64) # tensor([1., 2., 3.], dtype=torch.float64)\n",
    "\n",
    "# Change the array\n",
    "data = np.array([0, 0, 0])\n",
    "\n",
    "torch.Tensor(data)\n",
    "# tensor([1., 2., 3.]) -> Unchanged/Create an additional copy of the data in memory\n",
    "\n",
    "torch.tensor(data)\n",
    "# tensor([1, 2, 3], dtype=torch,int32) -> Unchanged/Create an additional copy of the data in memory --> Most used\n",
    "\n",
    "torch.as_tensor(data)\n",
    "# tensor([0, 0, 0], dtype=torch,int32) -> Changed/Share data --> Accepts any array\n",
    "# as_tensor doesn't work with built-in data structures like lists.\n",
    "\n",
    "torch.from_numpy(data)\n",
    "# tensor([0, 0, 0], dtype=torch,int32) -> Changed/Share data --> Accepts only numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([\n",
    "  [1, 1, 1, 1],\n",
    "  [2, 2, 2, 2],\n",
    "  [3, 3, 3, 3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# To find the shape:\n",
    "t.size()\n",
    "# torch.Size([3, 4])\n",
    "t.shape\n",
    "# torch.Size([3, 4])\n",
    "\n",
    "# To see thwe number of elements\n",
    "t.numel()\n",
    "# 12\n",
    "\n",
    "# Squeezing and unsqueezing a tensor\n",
    "t.reshape(1, 12).squeeze()\n",
    "# tensor([1., 1., 1., 2., 2... 3.])\n",
    "t.reshape(1, 12).squeeze().unsqueeze(dim=0) # tensor([[1., 1., 1., 2., 2... 3.]])\n",
    "\n",
    "# Flattening function:\n",
    "def flatten(my_tensor):\n",
    "\tmy_tensor = my_tensor.reshape(1, -1)\n",
    "\tmy_tensor = my_tensor.squeeze()\n",
    "\treturn my_tensor\n",
    "\n",
    "t = torch.tensor([\n",
    "  [1, 1, 1, 1],\n",
    "  [2, 2, 2, 2]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "flatten(t) # tensor([1, 1, 1, 1, 2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([\n",
    "\t[1, 1, 1, 1],\n",
    "\t[1, 1, 1, 1],\n",
    "\t[1, 1, 1, 1],\n",
    "\t[1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "\t[2, 2, 2, 2],\n",
    "\t[2, 2, 2, 2],\n",
    "\t[2, 2, 2, 2],\n",
    "\t[2, 2, 2, 2]\n",
    "])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "\t[3, 3, 3, 3],\n",
    "\t[3, 3, 3, 3],\n",
    "\t[3, 3, 3, 3],\n",
    "\t[3, 3, 3, 3]\n",
    "])\n",
    "\n",
    "# Concatenate\n",
    "t = torch.stack((t1, t2, t3))\n",
    "t.shape\n",
    "# torch.Size([3, 4, 4]) # batch of 3 tensors with the height and weight of 4\n",
    "\n",
    "# In order for a CNN to understand the imput (it expects also a color channel, we need to reshape the tensor):\n",
    "t = t.reshape(3, 1, 4 ,4)\n",
    "# 3 - image; 1 - color channel; 4 - rows of pixels; 4 - pixels per row\n",
    "# When working with CNNs, flattening is required. Flattening examples:\n",
    "t = torch.tensor([[\n",
    "\t[1, 1],\n",
    "\t[2, 2]],\n",
    "\t[[3, 3],\n",
    "\t [4, 4]]]\n",
    "\n",
    "t.reshape(1, -1)[0]\n",
    "t.reshape(-1)\n",
    "t.view(t.numel())\n",
    "t.flatten() # Flatten all the 3 images ( we don't want that)\n",
    "# tensor([1, 1, 2, 2, 3, 3, 4, 4])\n",
    "\n",
    "t.flatten(start_dim=1) # Flatten all the 3 images ( we don't want that)\n",
    "# tensor([[1, 1, 2, 2],\n",
    "#         [3, 3, 4, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Element wise operations:\n",
    "The 2 tensors needs to have the same shape to perform an element wise operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "\t[1, 2],\n",
    "\t[3, 4]])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "\t[9, 8],\n",
    "\t[7, 6]])\n",
    "\n",
    "t1 + t2 # or t1.add(t2)\n",
    "# 10.0 10.0\n",
    "# 10.0 10.0\n",
    "\n",
    "# Broadcasting:\n",
    "# t1 + 2 means that the 2 is broadcasted:\n",
    "np.broadcast_to(2, t1.shape)\n",
    "# array([2, 2],\n",
    "#   \t[2, 2]])\n",
    "\n",
    "a = array([10, 5, -1])\n",
    "print(a>0)\n",
    "# array([True, True, False], dtype=bool)\n",
    "\n",
    "t1 = torch.tensor([\n",
    "\t[1, 2],\n",
    "\t[3, 4]])\n",
    "t2 = torch.tensor([9, 8])\n",
    "np.broadcast_to(t2, t1.shape)\n",
    "# array([9, 8],\n",
    "#   \t[9, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction operations: Are the operations which reduces the number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "t = torch.tensor([\n",
    "\t[0, 1, 0],\n",
    "\t[2, 0, 2],\n",
    "\t[0, 3, 0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Sum\n",
    "t.sum() # tensor(8.)\n",
    "t.numel() # 9\n",
    "t.sum().numel # 1\n",
    "\n",
    "# Product, mean, std\n",
    "t.prod() # tensor(0.)\n",
    "t.mean() # tensor(0.8889)\n",
    "t.std() # tensor(1.1667)\n",
    "\n",
    "# Reduce a specific axis\n",
    "\n",
    "t = torch.tensor([\n",
    "\t[1, 1, 1, 1],\n",
    "\t[2, 2, 2, 2],\n",
    "\t[3, 3, 3, 3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Sum\n",
    "t.sum(dim=0) # tensor([6., 6., 6., 6.])\n",
    "t.sum(dim=1) # tensor([4., 8., 12.]) \n",
    "\n",
    "# Argmax function: Tells the index location of the maximum value inside a tensor\n",
    "t = torch.tensor([\n",
    "\t[1, 1, 1, 2],\n",
    "\t[3, 2, 2, 2],\n",
    "\t[4, 3, 1, 5]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t.max() # tensor(5.)\n",
    "t.argmax() # tensor(11)\n",
    "t.flatten() #  t = torch.tensor([1, 1, 1, 2, 3, 2, 2, 2, 4, 3, 1, 5])\n",
    "t.max(dim=0) # the max values followed by the indexes: (tensor([4., 3., 2., 5.]), tensor([2., 2., 1., 2.]))\n",
    "t.argmax(dim=0) # Only the indexes: tensor([2., 2., 1., 2.])\n",
    "\n",
    "# What we do when we need the value:\n",
    "\n",
    "t.max() # tensor(5.)\n",
    "t.max().item() # 5.0\n",
    "\n",
    "# What we do when we need the values:\n",
    "\n",
    "t.mean(dim=0) # tensor([2.6, 2., 1.3, 3.])\n",
    "t.mean(dim=0).tolist() # [2.6, 2., 1.3, 3.]\n",
    "t.mean(dim=0).numpy() # array([2.6, 2., 1.3, 3.], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST data set transformations:\n",
    "- PNG\n",
    "- trimming\n",
    "- resizing\n",
    "- sharpening\n",
    "- extending\n",
    "- negating\n",
    "- grayscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 steps of AI implementation\n",
    "- Prepare the data : ETL process (Extract; Transform ; Load)\n",
    "- Build the model\n",
    "- Train the model\n",
    "- Analyze the model's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # top level pytorch package\n",
    "import torchvision # acces to popular datasets and image transformation\n",
    "import torchvision.transforms as transforms # gives acces to common transformation to image processing\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "\troot='./data/FashionMNIST', # the location on disk where the data is located\n",
    "\ttrain=True, # the train set\n",
    "\tdownload=True, # tells the class to download the data if it's not present at the location we precised\n",
    "\ttransforms=transforms.Compose([ # Compose class allows us to do more than 1 transformations\n",
    "\t\ttransforms.ToTensor() # we need tensors\n",
    "\t])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.Dataloader(train_set, batch_size=10) # we can shuffle, have batch size ( quarrying )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "train_set.train_labels # tensor([9, 0, 0 ... 5])\n",
    "train_set.train_labels.bincount() # tensor([6000, 6000, ...., 6000]) the frequency of the data\n",
    "\n",
    "# One sample\n",
    "sample = next(iter(train_set))\n",
    "len(sample) # 2\n",
    "type(sample) # tuple\n",
    "image, label = sample\n",
    "image.shape # torch.Size([1, 28, 28])\n",
    "label.shape # torch.Size([]) ; scalar\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "\n",
    "# One batch\n",
    "batch = next(iter(train_loader))\n",
    "len(batch) # 2\n",
    "type(batch) # list\n",
    "images, labels = batch\n",
    "images.shape # torch.Size([10, 1, 28, 28])\n",
    "label.shape # torch.Size([10]) # rank one tensor\n",
    "grid = torchvision.utils.make_grid(images, nrows=10)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model: with torch.nn\n",
    "- In CNNs: kernel = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "\t\tself.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "\t\tself.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # fully connected / Dense / Linear layers\n",
    "\t\tself.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "\t\tself.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "\n",
    "\tdef forward(self, t):\n",
    "\t\tt = self.layer(t)\n",
    "\t\treturn t\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"overriten above nn.Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learnable parameters in nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "print(network)\n",
    "# Network(\n",
    "# \t(conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) # the stride is the sliding of the filter after each computation\n",
    "# \t(conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
    "# \t(fc1): Linear(in_features=192, out_features=120, bias=True) # the bias\n",
    "# \t(fc2): Linear(in_features=120, out_features=60, bias=True)\n",
    "# \t(out): Linear(in_features=60, out_features=10, bias=True)\n",
    "# )\n",
    "or\n",
    "print(network.conv1)\n",
    "# \t(conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To check the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network.conv1.weight)\n",
    "# tensor([[[[...................]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in network.parameters():\n",
    "    print(param.shape)\n",
    "# torch.Size([x, x, x, x])\n",
    "\n",
    "# For each layer, we have a bias tensor and a weight tensor \n",
    "for name, param in  network.named_parameters():\n",
    "    print(name, '\\t\\t', param.shape)\n",
    "# conv1.weight        torch.Size([6, 1, 5, 5])\n",
    "# conv1.bias          torch.Size([6])\n",
    "# conv2.weight        ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callable nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication:\n",
    "in_features = torch.tensor([1, 2 ,3 ,4], dtype=float32)\n",
    "weight_matrix = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3, 4, 5],\n",
    "    [3, 4, 5, 6]    \n",
    "], dtype=torch.float32)\n",
    "\n",
    "weight_matrix.matmul(in_features)\n",
    "# tensor([30., 40., 50.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "fc = nn.Linear(in_features=4, out_features=3)\n",
    "fc.weight nn.Parameter(weight_matrix)\n",
    "\n",
    "fc(in_features)\n",
    "# tensor([30.213, 40.213, 50.213], grad_fn=<AddBackward0>)\n",
    "\n",
    "# or\n",
    "fc = nn.Linear(in_features=4, out_features=3, bias=False)\n",
    "fc.weight nn.Parameter(weight_matrix)\n",
    "\n",
    "fc(in_features)\n",
    "# tensor([30., 40., 50.], grad_fn=<AddBackward0>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = Ax + b$\n",
    "- A: Weight matrix tensor\n",
    "- x: Input tensor\n",
    "- b: Bias tensor\n",
    "- y: Output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callable nn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "fc = nn.Linear(in_features=4, out_features=3)\n",
    "t = torch.tensor([1, 2, 3, 4], dtype=float32)\n",
    "\n",
    "output = fc(t)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def forward(self, t):\n",
    "    # (1) input layer\n",
    "    t = t\n",
    "\n",
    "    # (2) hidden conv layer\n",
    "    t = self.conv1(t) # application of the convolution\n",
    "    t = torch.nn.functional.relu(t) # the activatio function\n",
    "    t = torch.nn.functional.max.pool2d(t, kernel_size=2, stride=2) # max pooling operation\n",
    "    \n",
    "    # (3) hidden conv layer\n",
    "    t = self.conv2(t)\n",
    "    t = torch.nn.functional.relu(t)\n",
    "    t = torch.nn.functional.max.pool2d(t, kernel_size=2, stride=2)\n",
    "    \n",
    "    # (4) hidden linear layer\n",
    "    t = t.reshape(-1, 12 * 4 * 4) # flattening the input\n",
    "    t = self.fc1( t) # application of the linear layer\n",
    "    t = torch.nn.functional.relu(t)\n",
    "\n",
    "    # (5) hidden linear layer\n",
    "    t = self.fc2(t)\n",
    "    t = torch.nn.functional.relu(t)\n",
    "\n",
    "    # (6) output layer\n",
    "    t = self.out(t)\n",
    "    # t = torch.nn.functional.softmax(t, dim=1) # activation function for the output layer : it won't be used because of the training\n",
    "    # it will be using the cross_entrophy : it is used implicintly \n",
    "    \n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "\troot='./data/FashionMNIST',\n",
    "\ttrain=True,\n",
    "\tdownload=True,\n",
    "\ttransforms=transforms.Compose([\n",
    "\t\ttransforms.ToTensor()\n",
    "\t])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.Dataloader(train_set, batch_size=10) # we can shuffle, have batch size ( quarrying )\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "\t\tself.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "\t\tself.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # fully connected / Dense / Linear layers\n",
    "\t\tself.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "\t\tself.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t) # application of the convolution\n",
    "        t = torch.nn.functional.relu(t) # the activatio function\n",
    "        t = torch.nn.functional.max.pool2d(t, kernel_size=2, stride=2) # max pooling operation\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = torch.nn.functional.relu(t)\n",
    "        t = torch.nn.functional.max.pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4) # flattening the input\n",
    "        t = self.fc1(t) # application of the linear layer\n",
    "        t = torch.nn.functional.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = torch.nn.functional.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "\n",
    "network = Network()\n",
    "\n",
    "sample = next(iter(train_set))\n",
    "image, label = sample  \n",
    "image.unsqueeze(0).shape # torch.Size([1, 1, 28, 28]) : batch_size, in_channels, H, W \n",
    " \n",
    "pred = network(image.unsqueeze(0))\n",
    "print(pred) # tensor([[-0.313, 0.0123, 0.312. ......... -0.0023]])\n",
    "print(label) # 9\n",
    "print(pred.argmax(dim=1)) # tensor([2])\n",
    "torch.nn.functional.softmax(pred, dim=1) # valori intre 0 si 1\n",
    "torch.nn.functional.softmax(pred, dim=1).sum() # =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.Dataloader(train_set, batch_size=10)\n",
    "batch = next(iter(data_loader))\n",
    "images, labels = batch\n",
    "images.shape # torch.Size([10, 1, 28, 28])\n",
    "labels.shape # torch.Size([10])\n",
    "preds = network(images)\n",
    "preds.shape # torch.Size([10, 10])\n",
    "print(preds.argmax(dim=1)) # tensor([3, 3, 2, 4 .... , 4])\n",
    "print(labels) # tensor([2, 5, 7, 1 .... , 6])\n",
    "\n",
    "# making comparison:\n",
    "preds.argmax(dim=1).ex(labels) # tensor([1, 0, 0, 0 .... 1], dtype=uint8)\n",
    "preds.argmax(dim=1).ex(labels).sum() # tensor(3)\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).ex(labels).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  VS Code for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a CNN\n",
    "\n",
    "The training process can be broken down into 7 distinct steps:\n",
    "1. Get a batch from the training set\n",
    "2. Pass the batch through the network\n",
    "3. Calculate the loss ( difference between predicted and real) --> We use a loss function\n",
    "4. Calculate the gradient of the loss fct with respect to the the network's weights --> We use back propagation\n",
    "5. Update the weights using the gradient to reduce the loss. --> We use optimization algorithm\n",
    "6. Repeat steps 1-5 until one epoch is completed ( all the images from the training set are passed throght the network)\n",
    "7. Repeat steps 1-6 for as many epochs until we obtain the desired level of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim # for optimizer\n",
    "\n",
    "# For gradient tracking feature:\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)\n",
    "loss = torch.nn.functional.cross_entropy(preds, labels)\n",
    "loss.item() # 2.324214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network.conv1.weight.grad) # None\n",
    "\n",
    "loss.backward()\n",
    "print(network.conv1.weight.grad.shape) # torch.Size([6, 1, 5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the weights\n",
    "\n",
    "- SGD\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oprimizer = torch.optim.Adam(network.parameters(), lr=0.01) # lr = learning rate (hyper-parameter: you have to test and tune)\n",
    "# the network.parameters are the network's weights\n",
    "# lr=0.01 : how far in the minimum loss gradient direction you wanna step\n",
    "\n",
    "loss.item() # 2.31241\n",
    "get_num_correct(preds, labels) # 10\n",
    "\n",
    "optimizer.step() # Updating the weights: We want to step in the direction of the loss function's miniumum\n",
    "\n",
    "preds = network(images)\n",
    "loss = torch.nn.functional.cross_entropy(preds, labels)\n",
    "\n",
    "loss.item() # 2. 29321\n",
    "get_num_correct(preds, labels) # 19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortly: the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.Dataloader(train_set, batch_size=100)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "batch = next(iter(train_loader)) # 1. Get a batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = network(images) # 2. Pass the batch\n",
    "loss = torch.nn.functional.cross_entropy(preds, labels) # 3. Calculate loss\n",
    "\n",
    "loss.backward() # 4. Calculate Gradients\n",
    "optimizer.step() # 5. Update the Weights\n",
    "# __________________________________________\n",
    "print('loss1:', loss.item())\n",
    "preds = network(images)\n",
    "loss torch.nn.functional.cross_entropy(preds, labels)\n",
    "print('loss2:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop (complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self),\n",
    "    super().__init__()\n",
    "    self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "    self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "    \n",
    "    self.fc1 = torch.nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "    self.fc2 = torch.nn.Linear(in_features=120, out_features=60)\n",
    "    \n",
    "    self.out = torch.nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = torch.nn.functional.relu(t)\n",
    "        t = torch.nn.functional.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t = self.conv2(t)\n",
    "        t = torch.nn.functional.relu(t)\n",
    "        t = torch.nn.functional.max_pool2d(t, kernel_size, stride)\n",
    "        \n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = self.torch.nn.functional.relu(t)\n",
    "        \n",
    "        t = self.fc2(t)\n",
    "        t = self.torch.nn.functional.relu(t)\n",
    "        \n",
    "        t = self.out(t)\n",
    "#       t = self.torch.nn.functional.softmax(t)\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "root = './data',\n",
    "train=True,\n",
    "download=True,\n",
    "transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    ")\n",
    "\n",
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.Dataloader(train_set, batch_size=100)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = network(images)\n",
    "        loss = torch.nn.functional.cross_entropy(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad() # to reset the gradient (it adds it up by default)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\"epoch:\", epoch, \"total_correct:\", total_correct, \"loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_correct / len(train_set)) # 0.7798375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        \n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "        (all_preds, preds),\n",
    "        dim=0\n",
    "        )\n",
    "    return all_preds        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_loader  = torch.utils.data.Dataloader(train_set, batch_size=10000)\n",
    "train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_preds.shape) # torch.Size([60000, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_preds.require_grad) # True ()\n",
    "train_preds.grad # (nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.Dataloader(train_set, batch_size=10000)\n",
    "    train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_preds.requires_grad) # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.targets # tensor([9, 0 ,0, ....])\n",
    "train_preds.argmax(dim=1) tensor([9, 0, 0 ....])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stach(\n",
    "train_set.targets,\n",
    "train_preds.argmax(dim=1)), dim=1\n",
    ")\n",
    "stacked.shape # torch.Size([60000, 2])\n",
    "stacked \n",
    "#tensor([[9, 9],\n",
    "# [0, 0],\n",
    "# .\n",
    "# .])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked[0].tolist() # [9, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = torch.zeros(10, 10, dtype=torch.int64)\n",
    "\n",
    "for p in stacked:\n",
    "    true_label, predicted_label = p.tolist()\n",
    "    cmt[true_label, predicted_label] = cmt[true_label, predicted_label] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cmt)\n",
    "# tensor([[5324, 21, 42, 5 ,1 ....]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from resources.plotcm import plot_confusion_matrix # locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(train_set.targets, train_preds.argmax(din=1))\n",
    "names = ('T-Shirt/top', 'Trousers', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(cm, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing with tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make an alias for tensorboard:\n",
    "- alias tensorboard='python3 -m tensorboard.main'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start tensorboard server:\n",
    "- $ tensorboard --logdir=data/dasd/sfa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120) # display options for output\n",
    "torch.set_grad_enabled(True) # Already on by default\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
