{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow notes\n",
    "\n",
    "For large datasets, instalation of cuDNN (NVIDIAs Deep Neural Network library) is needed.\n",
    "\n",
    "Keras supports 3 different types of backends:\n",
    " - TensorFlow\n",
    " - Theano\n",
    " - CNTK\n",
    " \n",
    "For saving keras models on disk: HDF5 and h5py\n",
    "\n",
    "The samples for keras has to be a numpy array or a list of numpy arrays\n",
    "\n",
    "The labels has to be a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model: Optimizing the weights\n",
    "\n",
    "Stochastic Gradient Descent (SGD):\n",
    " - the most widely known optimizer\n",
    " - Objective: minimize the loss function (like mean squared error)\n",
    " cat 0.75, dog 0.25: error = 0 - 0.25 = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting: good at classifying the train data, but not good at classifying the test data\n",
    "- How do we know: when validation << training\n",
    "- Fighting again overfitting: more data and data augmentation (crop, zoom, rotating, flipping)\n",
    "- and reducing the complexity of the model (reducing layers or neurons from layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To scale data (from 0 to 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [23, 40, 12, 1, 0]\n",
    "n_data = np.array(data)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaled_data = scaler.fit_transform((n_data).reshape(-1,1))\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, input_shape=(1,), activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential() # most common model\n",
    "model.add(Dense(32, input_shape=(10,), activation='relu')) # The hidden layer: Dense is the most common one\n",
    "model.add(Dense(2, activation='softmax')) # the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check a model's details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizer.Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.loss = 'sparse_categorical_crossentropy'\n",
    "model.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the optimizer = Adam\n",
    "- loss function: mse mean squared error\n",
    "- lr = learning rate between 0.01 and 0.0001\n",
    "- metrics: what's printed out when the model it's trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised learning\n",
    " - Labeled data\n",
    "\n",
    "#### Unsupervised learning\n",
    " - clustering\n",
    "\n",
    "#### Semi-supervised learning\n",
    " - When only part of the data is labeled\n",
    " - Pseudo-labeling: Train with the labeled data -> label using the model the unlabeled data -> train the whole model with the new 100% labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(scaled_data, train_labels, batch_size=10, shuffle=True, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size = how many piceces of data to be sent to the model at once\n",
    "- how much output we want to see when we train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To chose the validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_set, train_labels, validation_split=0.2, batch_size=10, epochs=20, shuffle=True, verbose=1)\n",
    "# and the valid_set has to be this format:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We have explicitly the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_set, train_labels, validation_data=valid_set, batch_size=10, epochs=20, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the validation data has to be this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = [(sample, label), (sample, label) ... (sample, label)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a validation set\n",
    " - 1st way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = [(sample, label), ... , (sample, label)]\n",
    "model.fit(scaled_data, train_labels, validation_data=valid_set, batch_size=10, shuffle=True, verbose=2)\n",
    "model.fit(scaled_data, train_labels, validation_split=0.1, batch_size=10, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    " - Classic prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Rounded prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_prediction = model.predict_classes(test_data, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a model classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It saves:\n",
    "- the architecture of the model\n",
    "- the weights\n",
    "- the training configuration(compile): loss, optimizer\n",
    "- the state of the optimizer (resume training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a model as json string:\n",
    "- it saves only the architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('my_model.h5')\n",
    "# or\n",
    "new_model = tf.keras.models.model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a CNN data(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_path = 'cats and dogs/train'\n",
    "valid_path = 'cats and dogs/valid'\n",
    "test_path = 'cats and dogs/test'\n",
    "\n",
    "train_batches = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(train_path, target_size=(244, 244), classes=['dog', 'cat'], batch_size=10)\n",
    "valid_batches = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(valid_path, target_size=(244, 244), classes=['dog', 'cat'], batch_size=5)\n",
    "test_batches = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(test_path, target_size=(244, 244), classes=['dog', 'cat'], batch_size=5)\n",
    "test_batches.class_indices # to see the indices : {'cat':0, 'dog': 1, 'lizard': 2} : [1. 0. 0.] -> cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "\ttf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(244, 244, 3)), # number of output filter, the kernel size ( convo window), hight/width/channel(RGB)\n",
    "\ttf.keras.layers.Flatten(), # used to flat the output of the convo layer into a 1D tensor --> then fed into the dense layer\n",
    "\ttf.keras.layers.Dense(2, activation='softmax'),\n",
    "\t])\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_batches, steps_per_epoch=5, validation_data=validation_batches, validation_steps=4, epochs=5, verbose=2)\n",
    "# fit_generator - to fit the model batch by batch ( because of the ImageDataGenerator)\n",
    "# steps_per_epoch - total number of batches until a epoch is finished ( 50 / 10 = 5)\n",
    "# validation steps - the same as for steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing an already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = tf.keras.applications.vgg16.VGG16()\n",
    "# Because VGG is not a sequential model, we will take each layer and createa sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "for layer in vgg16_model.layers:\n",
    "    model.add(layer)\n",
    "model.layers.pop() # Delete that last 1000 outputs layer\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc, ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=10, # 10 radians\n",
    "                                                            width_shift_range=0.1, # 0.1 fraction of the entire width of the image\n",
    "                                                            height_shift_range=0.1,\n",
    "                                                            shear_range=0.15,\n",
    "                                                            zoom_range=0.1,\n",
    "                                                            channel_shift_range=10.,\n",
    "                                                            horizontal_flip=True)\n",
    "image_path = 'man.png'\n",
    "image = np.expand_dims(ndimage.imread(image_path), 0) # expand_dims to be compatible later on\n",
    "aug_iter = gen.flow(image) # generate batches of augmented images: takes the numpy data and generates back augmented data\n",
    "aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and access bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(4, input_shape=(1,), activation='relu', use_bias=True, bias_initializer='zeros'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    " ])\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable parameters:\n",
    " - weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "\n",
    "- use of filters: Edge detector, squares, corners, circles\n",
    "- Sliding through each 3x3 block of pixels: convolving through each block ( using a filter)\n",
    "- Type of filters:\n",
    "- -1: black, 1: white, 0: grey\n",
    "- -1 -1 -1\n",
    "-  1  1  1\n",
    "-  0  0  0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.convolutional import *\n",
    "from tensorflow.keras.core import Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero padding\n",
    "Initial<br>\n",
    "1 1 3<br>\n",
    "4 2 4<br>\n",
    "2 4 9<br>  \n",
    "Padding<br>\n",
    "0 0 0 0 0<br>\n",
    "0 1 1 3 0<br>\n",
    "0 4 2 4 0<br>\n",
    "0 2 4 9 0<br>\n",
    "0 0 0 0 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    Dense(16, activation='relu', input_shape(20, 20, 3)),\n",
    "    Conv2D(32, kernel_size(3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size(2, 2), strides=2, padding='valid'),\n",
    "    Conv2D(64, kernel_size(5, 5), activation='relu', padding='same'), # padding\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter size = how much we pool\n",
    "- stride = how much we move after we pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
