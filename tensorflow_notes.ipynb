{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow notes\n",
    "\n",
    "For large datasets, instalation of cuDNN (NVIDIAs Deep Neural Network library) is needed.\n",
    "\n",
    "Keras supports 3 different types of backends:\n",
    " - TensorFlow\n",
    " - Theano\n",
    " - CNTK\n",
    " \n",
    "For saving keras models on disk: HDF5 and h5py\n",
    "\n",
    "The samples for keras has to be a numpy array or a list of numpy arrays\n",
    "\n",
    "The labels has to be a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To scale data (from 0 to 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.575],\n",
       "       [1.   ],\n",
       "       [0.3  ],\n",
       "       [0.025],\n",
       "       [0.   ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [23, 40, 12, 1, 0]\n",
    "n_data = np.array(data)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaled_data = scaler.fit_transform((n_data).reshape(-1,1))\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-0e1d47f21638>\", line 3, in <module>\n",
      "    model = tf.keras.models.Sequential()\n",
      "AttributeError: module 'tensorflow' has no attribute 'keras'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    else:\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    name=name, loader=loader, origin=path)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    lines = index = None\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-0e1d47f21638>\", line 3, in <module>\n",
      "    model = tf.keras.models.Sequential()\n",
      "AttributeError: module 'tensorflow' has no attribute 'keras'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n",
      "    else:\n",
      "  File \"C:\\Users\\dbratu\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    name=name, loader=loader, origin=path)\n",
      "ImportError: DLL load failed: The specified module could not be found.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "To visualize the model:\n",
    "model.summary()\n",
    "#######################################################################################################\n",
    "# Creating a model\n",
    "\n",
    "#######################################################################################################\n",
    "# Compiling a model\n",
    "model.compile(tf.keras.optimizer.Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#######################################################################################################\n",
    "# Fitting a model\n",
    "model.fit(scaled_data, train_labels, batch_size=10, shuffle=True, verbose=2)\n",
    "#######################################################################################################\n",
    "# Create a validation set\n",
    "# 1st way\n",
    "valid_set = [(sample, label), ... , (sample, label)]\n",
    "model.fit(scaled_data, train_labels, validation_data=valid_set, batch_size=10, shuffle=True, verbose=2)\n",
    "model.fit(scaled_data, train_labels, validation_split=0.1, batch_size=10, shuffle=True, verbose=2)\n",
    "#######################################################################################################\n",
    "# Make a prediction\n",
    "# Classic prediction:\n",
    "predictions = model.predict(test_data, batch_size=10, verbose=0)\n",
    "# Rounded prediction:\n",
    "rounded_prediction = model.predict_classes(test_data, batch_size=10, verbose=0)\n",
    "# Confusion matrix to see the prediction accuracy\n",
    "confusion_matrix from sklearn.metrics\n",
    "#######################################################################################################\n",
    "# Save a model classic:\n",
    "model.save('my_model.h5')\n",
    "# it saves:\n",
    "# the architecture of the model\n",
    "# the weights\n",
    "# the training configuration(compile): loss, optimizer\n",
    "# the state of the optimizer (resume training)\n",
    "# Save a model as json string:\n",
    "model.to_json()\n",
    "# it saves only the architecture\n",
    "#######################################################################################################\n",
    "# Load the model:\n",
    "new_model = tf.keras.models.load_model('my_model.h5')\n",
    "or\n",
    "new_model = tf.keras.models.model_from_json(json_string)\n",
    "#######################################################################################################\n",
    "# See the weights:\n",
    "model.get_weights()\n",
    "#######################################################################################################\n",
    "# See the tree in CLI: $ tree\n",
    "#######################################################################################################\n",
    "# Prepare a CNN data(images)\n",
    "import tensorflow as tf\n",
    "\n",
    "train_path = 'cats and dogs/train'\n",
    "valid_path = 'cats and dogs/valid'\n",
    "test_path = 'cats and dogs/test'\n",
    "\n",
    "train_batches = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(train_path, target_size=(244, 244), classes=['dog', 'cat'], batch_size=10)\n",
    "valid_batches = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(valid_path, target_size=(244, 244), classes=['dog', 'cat'], batch_size=5)\n",
    "test_batches = tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory(test_path, target_size=(244, 244), classes=['dog', 'cat'], batch_size=5)\n",
    "test_batches.class_indices # to see the indices : {'cat':0, 'dog': 1, 'lizard': 2} : [1. 0. 0.] -> cat\n",
    "#######################################################################################################\n",
    "# create and train the model\n",
    "model = tf.keras.models.Sequential([\n",
    "\ttf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(244, 244, 3)), # number of output filter, the kernel size ( convo window), hight/width/channel(RGB)\n",
    "\ttf.keras.layers.Flatten(), # used to flat the output of the convo layer into a 1D tensor --> then fed into the dense layer\n",
    "\ttf.keras.layers.Dense(2, activation='softmax'),\n",
    "\t])\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_batches, steps_per_epoch=5, validation_data=validation_batches, validation_steps=4, epochs=5, verbose=2)\n",
    "# fit_generator - to fit the model batch by batch ( because of the ImageDataGenerator\n",
    "# steps_per_epoch - total number of batches until a epoch is finished ( 50 / 10 = 5)\n",
    "# validation steps - the same as for steps_per_epoch\n",
    "#######################################################################################################\n",
    "# Make a prediction\n",
    "predictions = model.predict_generator(test_batches, steps=1, vervose=0)\n",
    "#######################################################################################################\n",
    "# Importing an already trained model\n",
    "vgg16_model = tf.keras.applications.vgg16.VGG16()\n",
    "# Because VGG is not a sequential model, we will take each layer and createa sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "for layer in vgg16_model.layers:\n",
    "    model.add(layer)\n",
    "model.layers.pop() # Delete that last 1000 outputs layer\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "#######################################################################################################\n",
    "# Data augmentation\n",
    "from scipy import misc, ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=10, # 10 radians\n",
    "                                                            width_shift_range=0.1, # 0.1 fraction of the entire width of the image\n",
    "                                                            height_shift_range=0.1,\n",
    "                                                            shear_range=0.15,\n",
    "                                                            zoom_range=0.1,\n",
    "                                                            channel_shift_range=10.,\n",
    "                                                            horizontal_flip=True)\n",
    "image_path = 'man.png'\n",
    "image = np.expand_dims(ndimage.imread(image_path), 0) # expand_dims to be compatible later on\n",
    "aug_iter = gen.flow(image) # generate batches of augmented images: takes the numpy data and generates back augmented data\n",
    "aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(10)]\n",
    "#######################################################################################################\n",
    "# Initialize and access bias\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(4, input_shape=(1,), activation='relu', use_bias=True, bias_initializer='zeros'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    " ])\n",
    "model.get_weights()\n",
    "#######################################################################################################\n",
    "# Trainable parameters\n",
    "weights and biases\n",
    "# In a CNN\n",
    "same\n",
    "#######################################################################################################\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
