{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression\n",
    "#### The most popular regression algorithms are:\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Ordinary Least Squares Regression (OLSR)\n",
    "4. Stepwise Regression\n",
    "5. Multivariate Adaptive Regression Splines (MARS)\n",
    "6. Locally Estimated Scatterplot Smoothing (LOESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Linear regression\n",
    "<img src=\"data/images/linear_regression.png\" alt=\"xxx\" title=\"title\" width=260 height=260 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a form of predictive technique, used in trends, estimates, impact of price changes <br>\n",
    "$$y = mx + c$$\n",
    "- m = slope of the line\n",
    "- c = intercept of the line\n",
    "$$ m = \\frac{∑(x - x̄)(y - ӯ)}{∑(x - x̄)^2}$$\n",
    "The error (better when closer to 1)/Least squares/Residuals: <br>\n",
    "$$ R^2 = \\frac{∑(y_{pred} - ӯ)}{∑(y - ӯ)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Logistic Regression\n",
    "\n",
    "<img src=\"data/images/logistic_regression.png\" alt=\"xxx\" title=\"title\" width=260 height=260 />\n",
    "\n",
    "Logistic regression provides probabilities. <br>\n",
    "Logistic regression produces results in a binary format: <br>\n",
    "- 0 or 1\n",
    "- Yes or No\n",
    "- True or False\n",
    "- High or Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression doesn't have the concept of residuals, so it can't use least squares. <br>\n",
    "Instead, it uses the maximum likelihood.\n",
    "<img src=\"data/images/logistic_regression_likelihood.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Ordinary Least Squares Regression (OLSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/OLSR.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1.4. Stepwise Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is a tool used to \"pool\" the features which doesn't have such a big impact for our prediction model\n",
    "- Stepwise attempts to find the most important variables\n",
    "- E.g.: The price of a house is impacted by number of rooms and location, but not by color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works:\n",
    " - assume we have n independent variables\n",
    " - Step1: we create all possible n models: E(y) = B_0 + B_1 * x_k\n",
    "  - we choose the most signigicant xi\n",
    " - Step2: we create all possible n-1 models:E(y) = B_0 + B_1 * x_1 + B_2 * x_k\n",
    "  - we choose the most signigicant xi\n",
    " - We repeat the steps until the variable doesn't impact the model that much\n",
    "\n",
    "#### Methods to choose the best variable:\n",
    " - p value (smallest)\n",
    " - standard deviation\n",
    " - R squared\n",
    "\n",
    "#### Drawbacks:\n",
    "- Only linear terms are considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Multivariate Adaptive Regression Splines (MARS)\n",
    "\n",
    "- MARS models fits piecewise linear models\n",
    "- Hinge functions are used to \"cut\" the lines into sectors which could be easly shaped with linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/spline_regression.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/MARS.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/1vs2_cut_points.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Locally Estimated Scatterplot Smoothing (LOESS)\n",
    "\n",
    "\n",
    "#### Steps:\n",
    "- The data to be fitted\n",
    "<img src=\"data/images/LOESS1.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Divide the data into smaller blobs: 5 points\n",
    "\n",
    "<img src=\"data/images/LOESS2.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Within this window, each point will be focal point.\n",
    "- The focal point has the biggest weight, the next points has smaller weight proportional with the distance\n",
    "<img src=\"data/images/LOESS3.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After this, you'll have the fierst point of the fitted curve. It will be after that updated with respect of the distance between the curve and the actual point\n",
    "<img src=\"data/images/LOESS4.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here comes the 2nd weight\n",
    "<img src=\"data/images/LOESS5.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The curve after taking into consideration both weights (distance comparing to focal points and distance between the curve and the point)\n",
    "<img src=\"data/images/LOESS6.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional consideration:\n",
    "- Lines or parabolas\n",
    "<img src=\"data/images/LOESS7.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Difference between the two\n",
    "<img src=\"data/images/LOESS8.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The functions for the 2 weights:\n",
    "<img src=\"data/images/LOESS9.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification\n",
    "#### The most popular instance-based algorithms are:\n",
    "1. k-Nearest Neighbor (kNN)\n",
    "2. Learning Vector Quantization (LVQ)\n",
    "3. Self-Organizing Map (SOM)\n",
    "4. Locally Weighted Learning (LWL)\n",
    "5. Support Vector Machines (SVM) ★"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 k-Nearest Neighbor (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finds the nearest n neighbors, and it decides which class is the new element, seeing which neighbors has the most votes\n",
    "<img src=\"data/images/knn1.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When k=1, each training vector defines a region in space, defining a Voronoi partition of space\n",
    "<img src=\"data/images/knn2.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks:\n",
    "- For a two class problem, an odd k value must be chosen\n",
    "- k must not be a multiple of the number of classes\n",
    "- not so scalable (for large datasets, it can be a problem because many distances has to be calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Learning vector quantization (lvq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Support Vector Machines (SVM)\n",
    "\n",
    "- The extreme points of 2 classes are called Support Vectors\n",
    "- LSVM: linear suport vector machines: classes are linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bad threshold\n",
    "<img src=\"data/images/SVM1.png\" alt=\"xxx\" title=\"title\" width=760 height=760 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good threshold\n",
    "<img src=\"data/images/SVM2.png\" alt=\"xxx\" title=\"title\" width=760 height=760 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margin: is the minimum distance between threshold and the limits of the classes\n",
    "- If the threshold is in the middle, the margin is as large as it can be \n",
    "<img src=\"data/images/SVM3.png\" alt=\"xxx\" title=\"title\" width=760 height=760 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Low bias: when the threshold is robust (no missclassifications)\n",
    "- High bias: when we allow missclassifications --> Soft margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data is not classificable: we add another dimension --> kerneling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the mathematics possible, Support Vector Machines use Kernel functions to sytematically find Support Vector Classifiers in higher dimensions.\n",
    "\n",
    "Examples of Kernel functions:\n",
    " - Linear Kernel: x * y\n",
    " - Polynomial Kernel: (x * y)^d, d = 1, 2 ,3\n",
    " - Radial Kernel(Radial Basis Function kernel/RBF): finds Support Vector Classifiers in infinite dimensions: e^(-gama||x-y||2)\n",
    " <img src=\"data/images/SVM_radial_kernel.png\" alt=\"xxx\" title=\"title\" width=260 height=260 />\n",
    " - Sigmoid Kernel\n",
    " <img src=\"data/images/SVM_sigmoid.png\" alt=\"xxx\" title=\"title\" width=260 height=260 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages of SVM:\n",
    " - Poor performance when # features > # samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering\n",
    "#### The most popular clustering algorithms are:\n",
    "1. k-Means\n",
    "2. k-Medians\n",
    "3. Expectation Maximisation (EM)\n",
    "4. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The most popular Bayesian algorithms are:\n",
    "1. Naive Bayes ★\n",
    "2. Gaussian Naive Bayes\n",
    "3. Multinomial Naive Bayes\n",
    "4. Averaged One-Dependence Estimators (AODE)\n",
    "5. Bayesian Belief Network (BBN)\n",
    "6. Bayesian Network (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Algorithms\n",
    "1. Decision Tree ★\n",
    "2. Random Forest ★\n",
    "3. Dimensionality Reduction Algorithms\n",
    "4. Gradient Boosting algorithms\n",
    "5. GBM\n",
    "6. XGBoost\n",
    "7. LightGBM\n",
    "8. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Decision Tree\n",
    "It is a type of supervized learning algorithm\n",
    "\n",
    "Common Decision Tree Algorithms\n",
    " - Gini Index\n",
    " - Chi-Square\n",
    " - Information Gain\n",
    " - Reduction invariance \n",
    " \n",
    "Example:\n",
    "<img src=\"data/images/Decision_Trees8.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />\n",
    "<img src=\"data/images/Decision_Trees9.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The root\n",
    "<img src=\"data/images/Decision_Trees1.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees using binary data (Yes/No)\n",
    "<img src=\"data/images/Decision_Trees2.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finding out which one will be the root: Determine which one has the lowest impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini impurity\n",
    "\n",
    "<img src=\"data/images/Decision_Trees3.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />\n",
    "\n",
    "- for each leave\n",
    "     - Gini impurity = 1 - (probabiliy of yes)^2 - (probabiliy of no)^2\n",
    "     \n",
    "$$G.I._1 =1 - (\\frac{105}{105 + 39})^2 - (\\frac{39}{105 + 39})^2 = 0.395$$\n",
    "$$G.I._2 =1 - (\\frac{34}{34 + 125})^2 - (\\frac{125}{34 + 125})^2 = 0.336$$\n",
    "- After having these 2 impurities, we calculate the weighted average of them\n",
    "$$G.I._t =(\\frac{144}{144 + 159})*0.395 - (\\frac{159}{144 + 159})*0.336 = 0.364$$\n",
    "- Repeat this for each feature, and the feature with the smallest impurity will be the root\n",
    "- This is repeated until we reach the smallest impurity\n",
    "<img src=\"data/images/Decision_Trees4.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />\n",
    "- Here the impurity if we split one more time after chest pain is bigger comparing with what was before, so we make it a leave node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees numerical data\n",
    "<img src=\"data/images/Decision_Trees5.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />\n",
    "\n",
    " - First order the data asc\n",
    " - Calculate the average for adiacent values\n",
    " - Calculate the impurity for each average value\n",
    " - Choose the smallest impurity --> that will be the cut-off/threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees ranked/multiple choice data\n",
    "<img src=\"data/images/Decision_Trees6.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />\n",
    "\n",
    " - <= 1\n",
    " - <= 2\n",
    " - <= 3\n",
    " \n",
    " <img src=\"data/images/Decision_Trees7.png\" alt=\"xxx\" title=\"title\" width=460 height=460 />\n",
    " \n",
    " - Green\n",
    " - Red\n",
    " - Blue\n",
    " - Green or Red\n",
    " - Red or Blue\n",
    " - Blue or Green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Random Forest\n",
    " - One of the most powerful supervized learning algorithm\n",
    " - Capable of solving regression and classification tasks\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    " - Both classification and regresion tasks\n",
    " - Handles missing data\n",
    " - Won't overfit the model\n",
    " - Handle large datasets with higher dimensionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disdvantages:\n",
    " - Good at classification but not as good as for regression\n",
    " - Very little control ( Black box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a random forest:\n",
    " 1. Create a bootstrap dataset: Randomly selecting samples from the original dataset (one sample can be chosen more than once)\n",
    " 2. Create a decision tree using the bootstrapped data, but only using a random subset of freatures at each step ( each node of the tree)\n",
    " 3. This is a tree. Repeat this hundreds of times to create a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is it used:\n",
    " - With a new data, to be predicted, it is passed through all the trees from the random forest.\n",
    " - The option with the most votes will be chosen as class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the accuracy is calculated: the data which is not included in the bootstrapped data, called out-of-bag data, it's passed through the trees and labeled.\n",
    "\n",
    "Accuracy = the proportion of the out-of-bag samples which were correctly labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration:\n",
    " - Build the random forest\n",
    " - Estimate the accuracy\n",
    " - Change the number of variables used at building the random forest\n",
    " - Estimate again\n",
    " - Choose the best model/forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
